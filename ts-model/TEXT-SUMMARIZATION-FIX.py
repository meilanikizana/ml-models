# -*- coding: utf-8 -*-
"""Untitled16_(1)_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hLyVJXe2_Jm2f1VxT1B10NEneXr0-G18
"""

!pip install --upgrade typing-extensions==4.11 numpy==1.25.2

!pip install numpy==1.25.2 --upgrade --quiet
import os
os.kill(os.getpid(), 9)  # Restart runtime setelah upgrade numpy

"""# BAGIAN 1: IMPORT LIBRARY


"""

print("=" * 60)
print("IMPORT LIBRARY")
print("=" * 60)

# Library dasar
import numpy as np
import pandas as pd
import re
import os
import random
import pickle
import json
import time
import zipfile
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from collections import Counter

# Library NLP
try:
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    print("NLTK berhasil diimport")
except:
    print("NLTK tidak tersedia, akan menggunakan fungsi tokenisasi alternatif")

# Library scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Library TensorFlow/Keras
try:
    import tensorflow as tf
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import (
        Input, LSTM, GRU, Dense, Embedding, Dropout,
        Bidirectional, Attention, AdditiveAttention,
        LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
    )
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
    from tensorflow.keras.utils import to_categorical

    print(f"TensorFlow versi {tf.__version__} berhasil diimport")

    # Cek ketersediaan GPU
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        print(f"GPU tersedia: {len(gpus)} GPU terdeteksi")
        # Batasi penggunaan memori GPU
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    else:
        print("GPU tidak tersedia, menggunakan CPU")

except:
    print("TensorFlow/Keras tidak tersedia")
    raise ImportError("TensorFlow/Keras diperlukan untuk menjalankan kode ini")

# Set random seeds untuk reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

warnings.filterwarnings('ignore')

print("Setup awal selesai!")

"""# BAGIAN 2: FUNGSI PEMROSESAN TEKS"""

def clean_text(text):
    """Pembersihan teks yang lebih robust"""
    if not isinstance(text, str):
        return ""

    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text)

    # Remove special characters but keep punctuation
    text = re.sub(r'[^\w\s.,!?;:\-()]', '', text)

    # Remove multiple punctuation
    text = re.sub(r'[.,!?;:]{2,}', '.', text)

    return text.strip()

def simple_sentence_tokenize(text):
    """Tokenisasi kalimat sederhana tanpa NLTK"""
    text = text.replace('\n', ' ').strip()

    sentences = []
    for part in re.split(r'(?<=[.!?])\s+', text):
        if part.strip():
            sentences.append(part.strip())

    if not sentences:
        return [text]

    return sentences

def tokenize_words(text):
    """Tokenisasi kata sederhana tanpa NLTK"""
    text = text.lower()
    # Bersihkan teks
    text = re.sub(r'[^\w\s]', ' ', text)
    # Split kata-kata
    return [word for word in text.split() if word.strip()]

"""# BAGIAN 3: EKSTRAKSI DAN PEMROSESAN DATASET INDOSUM"""

def extract_zip(zip_path, extract_dir="extracted_files"):
    """Ekstrak file ZIP IndoSum dataset"""
    if os.path.exists(extract_dir):
        print(f"Direktori {extract_dir} sudah ada, menggunakan direktori tersebut")
        return extract_dir

    if not os.path.exists(zip_path):
        print(f"File {zip_path} tidak ditemukan. File yang tersedia:")
        available_files = [f for f in os.listdir() if f.endswith('.zip')]
        print(available_files)
        return None

    os.makedirs(extract_dir, exist_ok=True)

    print(f"Mengekstrak {zip_path}...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    print(f"Ekstraksi selesai ke {extract_dir}")
    return extract_dir

def find_jsonl_files(directory):
    """Cari file JSONL dalam direktori"""
    jsonl_files = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith('.jsonl'):
                jsonl_files.append(os.path.join(root, file))

    print(f"Ditemukan {len(jsonl_files)} file JSONL:")
    for i, file_path in enumerate(jsonl_files):
        print(f"  {i+1}. {file_path}")

    return jsonl_files

def read_jsonl(file_path):
    """Baca file JSONL"""
    data = []
    print(f"Membaca file: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f):
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"âš  Error pada baris {line_num}: {e}")
                continue

    print(f"Berhasil membaca {len(data)} entri")
    return data

def process_indosum_data(data, max_samples=None):
    """Proses data IndoSum menjadi pasangan teks-ringkasan"""
    pairs = []

    print(f"Memproses {len(data)} entri data IndoSum...")

    for item in tqdm(data, desc="Processing"):
        try:
            if "paragraphs" in item and "summary" in item:
                full_text = ""
                for paragraph in item["paragraphs"]:
                    for sentence in paragraph:
                        if isinstance(sentence, list):
                            full_text += " ".join(sentence) + " "
                        else:
                            full_text += str(sentence) + " "

                # Gabungkan ringkasan
                summary_text = ""
                for sentence in item["summary"]:
                    if isinstance(sentence, list):
                        summary_text += " ".join(sentence) + " "
                    else:
                        summary_text += str(sentence) + " "

                # Bersihkan teks
                full_text = clean_text(full_text)
                summary_text = clean_text(summary_text)

                # Filter data berkualitas
                if (len(full_text.split()) >= 50 and len(summary_text.split()) >= 10 and
                    len(full_text.split()) <= 500 and len(summary_text.split()) <= 100):
                    pairs.append((full_text, summary_text))

                if max_samples and len(pairs) >= max_samples:
                    break

        except Exception as e:
            continue

    print(f"Berhasil memproses {len(pairs)} pasangan teks-ringkasan berkualitas")
    return pairs

# Ekstrak dan proses dataset IndoSum
print("=" * 60)
print("EKSTRAKSI DAN PENGOLAHAN DATASET INDOSUM")
print("=" * 60)

zip_filename = "modelDICODINGML.zip"

try:
    # Ekstrak dataset
    extract_dir = extract_zip(zip_filename)

    if not extract_dir:
        raise Exception("Gagal mengekstrak dataset")

    # Cari file JSONL
    jsonl_files = find_jsonl_files(extract_dir)

    if not jsonl_files:
        raise Exception("Tidak ada file JSONL ditemukan")

    # Identifikasi file train dan test
    train_files = [f for f in jsonl_files if "train" in f.lower()]
    test_files = [f for f in jsonl_files if "test" in f.lower()]

    if not train_files:
        train_files = jsonl_files[:1]
    if not test_files:
        test_files = jsonl_files[-1:] if len(jsonl_files) > 1 else train_files

    print(f"File training: {train_files[0]}")
    print(f"File testing: {test_files[0]}")

    # Baca data (batasi untuk demo)
    train_data = read_jsonl(train_files[0])
    test_data = read_jsonl(test_files[0])

    # Proses data dengan batasan untuk demo
    print("Memproses data training...")
    train_pairs = process_indosum_data(train_data, max_samples=2000)

    print("Memproses data testing...")
    test_pairs = process_indosum_data(test_data, max_samples=500)

    if len(train_pairs) == 0:
        raise Exception("Tidak ada data training yang valid")

    print(f"Data training: {len(train_pairs)} pasangan")
    print(f"Data testing: {len(test_pairs)} pasangan")

except Exception as e:
    print(f"Error: {str(e)}")
    print("Menggunakan data dummy untuk demo...")

    # Data dummy Indonesia
    train_pairs = [
        ("Pemerintah Indonesia mengumumkan kebijakan baru dalam bidang pendidikan yang bertujuan untuk meningkatkan kualitas pembelajaran di seluruh nusantara. Kebijakan ini mencakup peningkatan fasilitas sekolah, pelatihan guru, dan pengembangan kurikulum yang lebih modern dan relevan dengan kebutuhan zaman. Diharapkan dengan kebijakan ini, Indonesia dapat bersaing di kancah internasional dalam bidang pendidikan.",
         "Pemerintah Indonesia mengumumkan kebijakan pendidikan baru untuk meningkatkan kualitas pembelajaran melalui peningkatan fasilitas, pelatihan guru, dan pengembangan kurikulum modern."),

        ("Teknologi kecerdasan buatan atau artificial intelligence semakin berkembang pesat di Indonesia. Banyak perusahaan teknologi mulai mengadopsi AI untuk berbagai keperluan bisnis, mulai dari customer service, analisis data, hingga pengembangan produk. Pemerintah juga mendukung pengembangan AI dengan menyediakan berbagai program pelatihan dan fasilitas riset bagi para developer dan peneliti.",
         "Teknologi AI berkembang pesat di Indonesia dengan adopsi oleh perusahaan dan dukungan pemerintah melalui program pelatihan dan fasilitas riset."),

        ("Sektor pariwisata Indonesia mengalami pemulihan yang signifikan setelah pandemi COVID-19. Jumlah wisatawan domestik dan mancanegara mulai meningkat, terutama ke destinasi-destinasi populer seperti Bali, Yogyakarta, dan Lombok. Pemerintah terus berupaya mempromosikan pariwisata Indonesia melalui berbagai program dan event internasional untuk menarik lebih banyak wisatawan.",
         "Sektor pariwisata Indonesia pulih signifikan pasca pandemi dengan peningkatan wisatawan ke Bali, Yogyakarta, dan Lombok serta promosi pemerintah."),
    ] * 100  # Duplikasi untuk demo

    test_pairs = train_pairs[:50]

# Split data training menjadi train dan validation
train_texts = [pair[0] for pair in train_pairs]
train_summaries = [pair[1] for pair in train_pairs]

# Split 80-20 untuk train-validation
X_train, X_val, y_train, y_val = train_test_split(
    train_texts, train_summaries, test_size=0.2, random_state=SEED
)

print(f"Final dataset split:")
print(f"  - Training: {len(X_train)} samples")
print(f"  - Validation: {len(X_val)} samples")
print(f"  - Testing: {len(test_pairs)} samples")

"""# BAGIAN 4: TOKENIZATION DAN PREPROCESSING"""

print("\n" + "=" * 60)
print("TOKENIZATION DAN PREPROCESSING")
print("=" * 60)

# Konfigurasi tokenizer
MAX_VOCAB_SIZE = 10000
MAX_INPUT_LEN = 200
MAX_OUTPUT_LEN = 50
EMBEDDING_DIM = 128

# Tokenizer untuk input (artikel)
input_tokenizer = Tokenizer(
    num_words=MAX_VOCAB_SIZE,
    oov_token="<OOV>",
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
)
input_tokenizer.fit_on_texts(X_train + X_val)

# Tokenizer untuk output (ringkasan) dengan special tokens
output_texts = ["<START> " + text + " <END>" for text in y_train + y_val]
output_tokenizer = Tokenizer(
    num_words=MAX_VOCAB_SIZE,
    oov_token="<OOV>",
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'
)
output_tokenizer.fit_on_texts(output_texts)

print(f"Input vocabulary size: {len(input_tokenizer.word_index)}")
print(f"Output vocabulary size: {len(output_tokenizer.word_index)}")

# Convert texts to sequences
X_train_seq = input_tokenizer.texts_to_sequences(X_train)
X_val_seq = input_tokenizer.texts_to_sequences(X_val)
X_test_seq = input_tokenizer.texts_to_sequences([pair[0] for pair in test_pairs])

# Pad sequences
X_train_seq = pad_sequences(X_train_seq, maxlen=MAX_INPUT_LEN, padding='post')
X_val_seq = pad_sequences(X_val_seq, maxlen=MAX_INPUT_LEN, padding='post')
X_test_seq = pad_sequences(X_test_seq, maxlen=MAX_INPUT_LEN, padding='post')

print(f"Training sequences shape: {X_train_seq.shape}")
print(f"Validation sequences shape: {X_val_seq.shape}")
print(f"Testing sequences shape: {X_test_seq.shape}")

"""# BAGIAN 5: MODEL RINGKASAN TEKS"""

print("\n" + "=" * 60)
print("MEMBANGUN MODEL RINGKASAN TEKS")
print("=" * 60)

class TextSummarizerModel:
    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim=128):
        """Inisialisasi model summarization"""
        self.input_vocab_size = input_vocab_size
        self.output_vocab_size = output_vocab_size
        self.embedding_dim = embedding_dim
        self.model = None

    def build_model(self):
        """Build model untuk klasifikasi kalimat penting"""
        print("Membangun model TensorFlow...")

        # Input layer
        inputs = Input(shape=(MAX_INPUT_LEN,))

        # Embedding layer
        embedding = Embedding(
            input_dim=self.input_vocab_size,
            output_dim=self.embedding_dim,
            mask_zero=True
        )(inputs)

        # Bidirectional LSTM untuk pemahaman konteks yang lebih baik
        lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)

        # Global pooling untuk mendapatkan representasi kalimat
        pooled = GlobalAveragePooling1D()(lstm)

        # Dense layers dengan dropout untuk menghindari overfitting
        x = Dense(128, activation='relu')(pooled)
        x = Dropout(0.3)(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.3)(x)

        # Output layer - skor penting untuk kalimat
        outputs = Dense(1, activation='sigmoid')(x)

        # Compile model
        self.model = Model(inputs, outputs)
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        print("Model berhasil dibangun!")
        return self.model

    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=5, batch_size=32):
        """Train model dengan data"""
        print("Training model...")

        y_train_importance = np.zeros((len(X_train), 1))
        y_train_importance[:, 0] = 1

        if X_val is not None and y_val is not None:
            y_val_importance = np.zeros((len(X_val), 1))
            y_val_importance[:, 0] = 1

            # Callbacks for better training
            callbacks = [
                EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)
            ]

            # Train with validation
            history = self.model.fit(
                X_train, y_train_importance,
                validation_data=(X_val, y_val_importance),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1
            )
        else:
            # Train without validation
            history = self.model.fit(
                X_train, y_train_importance,
                epochs=epochs,
                batch_size=batch_size,
                verbose=1
            )

        print("Training selesai!")
        return history

    def save(self, model_path="text_summarizer_model.keras"):
        """Simpan model ke disk"""
        if self.model is not None:
            self.model.save(model_path)
            print(f"Model berhasil disimpan ke {model_path}")
        else:
            print("Model belum dibangun, tidak ada yang disimpan")

    def predict_sentence_importance(self, sentences):
        """Memprediksi kalimat mana yang penting dalam teks"""
        if self.model is None:
            raise ValueError("Model belum dibangun atau dilatih")

        # Tokenize dan pad setiap kalimat
        sequences = []
        for sentence in sentences:
            seq = input_tokenizer.texts_to_sequences([sentence])
            if seq[0]:
                padded_seq = pad_sequences(seq, maxlen=MAX_INPUT_LEN, padding='post')
                sequences.append(padded_seq)
            else:
                # Jika tokenisasi gagal, beri nilai 0
                sequences.append(np.zeros((1, MAX_INPUT_LEN)))

        # Prediksi skor penting untuk setiap kalimat
        importance_scores = []
        for seq in sequences:
            score = self.model.predict(seq, verbose=0)[0][0]
            importance_scores.append(score)

        return importance_scores

    def summarize(self, text, max_sentences=3):
        """Menghasilkan ringkasan dari teks"""
        # Preprocessing
        cleaned_text = clean_text(text)
        if not cleaned_text:
            return "Teks tidak valid atau kosong."

        # Tokenisasi kalimat
        try:
            sentences = sent_tokenize(cleaned_text)
        except Exception as e:
            print(f"Error dengan NLTK sent_tokenize: {e}")
            sentences = simple_sentence_tokenize(cleaned_text)

        # Jika teks sudah pendek, return as is
        if len(sentences) <= max_sentences:
            return cleaned_text

        # Gunakan model untuk memprediksi kalimat penting
        try:
            importance_scores = self.predict_sentence_importance(sentences)

            # Ambil indeks kalimat dengan skor tertinggi
            top_indices = np.argsort(importance_scores)[-max_sentences:]
            top_indices = sorted(top_indices)  # Urutkan berdasarkan posisi asli

            # Ambil kalimat-kalimat penting
            summary_sentences = [sentences[i] for i in top_indices]

            return " ".join(summary_sentences)

        except Exception as e:
            print(f"Error saat prediksi model: {e}")

            # Fallback: Strategi ekstraktif sederhana (kalimat pertama, tengah, terakhir)
            summary_sentences = [sentences[0]]  # Kalimat pertama selalu penting

            if max_sentences >= 2:
                summary_sentences.append(sentences[-1])  # Kalimat terakhir

            if max_sentences >= 3 and len(sentences) > 2:
                # Tambahkan kalimat tengah
                middle_idx = len(sentences) // 2
                if sentences[middle_idx] not in summary_sentences:
                    summary_sentences.insert(1, sentences[middle_idx])

            return " ".join(summary_sentences)

# Inisialisasi dan build model
print("Membangun model...")
summarization_model = TextSummarizerModel(
    input_vocab_size=len(input_tokenizer.word_index) + 1,
    output_vocab_size=len(output_tokenizer.word_index) + 1,
    embedding_dim=EMBEDDING_DIM
)

model = summarization_model.build_model()
print("Model berhasil dibangun!")
model.summary()

"""# BAGIAN 6: TRAINING MODEL"""

print("\n" + "=" * 60)
print("TRAINING MODEL")
print("=" * 60)

# Callbacks
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        'best_summarization_model.h5',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=0.0001,
        verbose=1
    )
]

# Training target: kalimat pertama adalah penting
y_train_importance = np.zeros((len(X_train_seq), 1))
y_train_importance[:, 0] = 1  # Mark first sentence as important

y_val_importance = np.zeros((len(X_val_seq), 1))
y_val_importance[:, 0] = 1

print("Memulai training...")
print(f"Training samples: {len(X_train_seq)}")
print(f"Validation samples: {len(X_val_seq)}")

try:
    history = model.fit(
        X_train_seq, y_train_importance,
        validation_data=(X_val_seq, y_val_importance),
        epochs=10,  # Reduced untuk demo
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    print("Training selesai!")

    # Plot training history
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Save model
    summarization_model.save("text_summarizer_model.keras")
    print("Model berhasil disimpan ke text_summarizer_model.keras")

except Exception as e:
    print(f"Error during training: {str(e)}")
    print("Menggunakan pendekatan alternatif...")

    # Jika training gagal, buat model alternatif yang lebih sederhana
    try:
        print("Mencoba model lebih sederhana...")

        # Model sederhana
        simple_model = Sequential([
            Embedding(len(input_tokenizer.word_index) + 1, 64, input_length=MAX_INPUT_LEN),
            LSTM(32),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ])

        simple_model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        # Train model sederhana
        simple_model.fit(
            X_train_seq, y_train_importance,
            epochs=5,
            batch_size=32,
            verbose=1
        )

        # Save model
        simple_model.save("text_summarizer_model_simple.keras")
        print("Model sederhana berhasil disimpan ke text_summarizer_model_simple.keras")

        # Assign to main model
        summarization_model.model = simple_model

    except Exception as e2:
        print(f"Error dengan model alternatif: {str(e2)}")
        print("Akan menggunakan pendekatan ekstraktif tanpa model")

"""# BAGIAN 7: IMPLEMENTASI FUNGSI INFERENSI"""

print("\n" + "=" * 60)
print("IMPLEMENTASI FUNGSI INFERENSI (PERBAIKAN)")
print("=" * 60)

def summarize_text(text, max_sentences=3):
    """Fungsi utama untuk meringkas teks"""

    try:
        # Preprocessing
        cleaned_text = clean_text(text)
        if not cleaned_text:
            return "Teks tidak valid atau kosong."

        # Tokenisasi kalimat
        try:
            sentences = sent_tokenize(cleaned_text)
            print("NLTK sent_tokenize berhasil digunakan")
        except Exception as e:
            print(f"Error dengan NLTK sent_tokenize: {e}")
            # Fallback ke tokenisasi sederhana
            sentences = simple_sentence_tokenize(cleaned_text)
            print("Menggunakan tokenisasi sederhana sebagai fallback")

        # Log jumlah kalimat untuk debugging
        print(f"Jumlah kalimat dalam teks: {len(sentences)}")

        if len(sentences) <= max_sentences:
            return cleaned_text

        if hasattr(summarization_model, 'model') and summarization_model.model is not None:
            try:
                print("Menggunakan model untuk prediksi...")
                # Tokenize dan pad setiap kalimat
                sentence_sequences = []
                for sentence in sentences:
                    seq = input_tokenizer.texts_to_sequences([sentence])
                    if len(seq[0]) > 0:  # Skip kalimat kosong
                        padded_seq = pad_sequences(seq, maxlen=MAX_INPUT_LEN, padding='post')
                        sentence_sequences.append(padded_seq)
                    else:
                        # Dummy sequence untuk kalimat yang tidak bisa ditokenisasi
                        sentence_sequences.append(np.zeros((1, MAX_INPUT_LEN)))

                # Prediksi importance scores
                importance_scores = []
                for i, seq in enumerate(sentence_sequences):
                    try:
                        score = summarization_model.model.predict(seq, verbose=0)[0][0]
                        importance_scores.append(score)
                        print(f"  - Kalimat {i+1}: Skor {score:.4f}")
                    except Exception as e:
                        print(f"Error saat prediksi kalimat {i+1}: {e}")
                        importance_scores.append(0.0)  # Default low score

                if len(importance_scores) == len(sentences):
                    top_indices = np.argsort(importance_scores)[-max_sentences:]
                    top_indices = sorted(top_indices)  # Urutkan berdasarkan posisi asli
                    summary_sentences = [sentences[i] for i in top_indices]
                    return " ".join(summary_sentences)

                print("Prediksi model tidak lengkap, menggunakan fallback...")
            except Exception as e:
                print(f"Error saat menggunakan model: {e}")
                print("Menggunakan pendekatan ekstraktif sebagai fallback...")
        else:
            print("Model tidak tersedia, menggunakan pendekatan ekstraktif...")

        # Pendekatan ekstraktif: kalimat pertama, tengah, dan terakhir
        summary_sentences = [sentences[0]]  # Kalimat pertama selalu penting

        if max_sentences >= 2:
            summary_sentences.append(sentences[-1])  # Kalimat terakhir

        if max_sentences >= 3 and len(sentences) > 2:
            middle_idx = len(sentences) // 2
            if sentences[middle_idx] not in summary_sentences:
                summary_sentences.insert(1, sentences[middle_idx])

        # Urutkan berdasarkan posisi asli dalam teks
        positions = []
        for sent in summary_sentences:
            for i, s in enumerate(sentences):
                if sent == s:
                    positions.append(i)
                    break

        sorted_pairs = sorted(zip(positions, summary_sentences))
        ordered_summary = [pair[1] for pair in sorted_pairs]

        return " ".join(ordered_summary)

    except Exception as e:
        print(f"Error dalam ringkasan: {str(e)}")
        # Ultra basic fallback
        words = text.split()
        if len(words) <= 50:
            return text
        else:
            # Return first 1/3 of text as summary
            summary_length = len(words) // 3
            return " ".join(words[:summary_length])

# Tes fungsi dengan contoh sederhana
print("Tes fungsi summarize_text dengan contoh sederhana...")
test_text = """
Teknologi kecerdasan buatan atau artificial intelligence semakin berkembang pesat di Indonesia.
Banyak perusahaan teknologi mulai mengadopsi AI untuk berbagai keperluan bisnis, mulai dari
customer service, analisis data, hingga pengembangan produk. Pemerintah juga mendukung
pengembangan AI dengan menyediakan berbagai program pelatihan dan fasilitas riset bagi
para developer dan peneliti. Dengan dukungan ini, diharapkan Indonesia dapat menjadi salah
satu pusat pengembangan AI di Asia Tenggara dalam beberapa tahun ke depan.
"""

print("\nTeks contoh:")
print(test_text)

print("\nHasil ringkasan:")
test_summary = summarize_text(test_text)
print(test_summary)

# Function untuk menghitung ROUGE-1 F1 score
def calculate_rouge_1(reference, candidate):
    """Hitung ROUGE-1 F1 score dengan metode yang lebih handal"""
    try:
        # Tokenisasi manual untuk kehandalan
        def tokenize_text(text):
            # Lowercase dan bersihkan teks
            text = text.lower()
            text = re.sub(r'[^\w\s]', ' ', text)
            # Split dan filter kata-kata kosong
            return [word for word in text.split() if word.strip()]

        # Tokenisasi
        ref_tokens = set(tokenize_text(reference))
        cand_tokens = set(tokenize_text(candidate))

        if len(ref_tokens) == 0:
            return 0.0

        # Hitung overlap
        overlap = ref_tokens.intersection(cand_tokens)

        # Hitung precision dan recall
        precision = len(overlap) / len(cand_tokens) if len(cand_tokens) > 0 else 0
        recall = len(overlap) / len(ref_tokens)

        # Hitung F1 score
        if precision + recall == 0:
            return 0.0

        f1 = 2 * precision * recall / (precision + recall)
        return f1
    except Exception as e:
        print(f"Error saat menghitung ROUGE: {e}")
        return 0.0

# Test pada beberapa sample data
if test_pairs and len(test_pairs) > 0:
    print("\nTesting model pada sample data...")

    # Ambil maksimal 5 samples
    test_samples = test_pairs[:5]
    rouge_scores = []

    for i, (original_text, reference_summary) in enumerate(test_samples):
        print(f"\nTest Sample {i+1}:")
        print(f"Original: {original_text[:200]}...")
        print(f"Reference: {reference_summary}")

        # Generate summary
        print(f"\nMemproses ringkasan untuk sampel {i+1}...")
        generated_summary = summarize_text(original_text)

        print(f"Generated: {generated_summary}")

        # Calculate ROUGE score
        rouge_score = calculate_rouge_1(reference_summary, generated_summary)
        rouge_scores.append(rouge_score)
        print(f"ROUGE-1 F1: {rouge_score:.3f}")

    # Hitung rata-rata skor ROUGE
    if rouge_scores:
        avg_rouge = sum(rouge_scores) / len(rouge_scores)
        print(f"\nAverage ROUGE-1 F1 Score: {avg_rouge:.3f}")
    else:
        print("\nTidak ada skor ROUGE yang dihitung.")
else:
    print("\nTidak ada test data yang tersedia untuk evaluasi.")

"""# BAGIAN 8: SIMPAN MODEL DAN TOKENIZER"""

print("\n" + "=" * 60)
print("MENYIMPAN MODEL DAN TOKENIZER")
print("=" * 60)

try:
    if hasattr(summarization_model, 'model') and summarization_model.model is not None:
        model_path = "text_summarizer_model.keras"
        summarization_model.model.save(model_path)
        print(f"Model berhasil disimpan ke {model_path}")
    else:
        print("Model tidak tersedia untuk disimpan")
except Exception as e:
    print(f"Error saat menyimpan model: {str(e)}")
    try:
        model_path_h5 = "text_summarizer_model.h5"
        if hasattr(summarization_model, 'model') and summarization_model.model is not None:
            summarization_model.model.save(model_path_h5)
            print(f"Model berhasil disimpan sebagai {model_path_h5}")
    except Exception as e2:
        print(f"Error pada alternatif penyimpanan: {str(e2)}")

# Simpan tokenizer
try:
    print("Menyimpan tokenizer...")

    with open('input_tokenizer.pickle', 'wb') as handle:
        pickle.dump(input_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
        print("Input tokenizer berhasil disimpan")

    with open('output_tokenizer.pickle', 'wb') as handle:
        pickle.dump(output_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
        print("Output tokenizer berhasil disimpan")

except Exception as e:
    print(f"Error saat menyimpan tokenizer: {str(e)}")

# Simpan fungsi pemrosesan teks
try:
    print("Menyimpan fungsi pemrosesan teks...")

    with open("text_processing.py", "w") as f:
        f.write("""
import re

def clean_text(text):
    \"\"\"Pembersihan teks yang lebih robust\"\"\"
    if not isinstance(text, str):
        return ""

    # Remove extra whitespaces
    text = re.sub(r'\\s+', ' ', text)

    # Remove special characters but keep punctuation
    text = re.sub(r'[^\\w\\s.,!?;:\\-()]', '', text)

    # Remove multiple punctuation
    text = re.sub(r'[.,!?;:]{2,}', '.', text)

    return text.strip()

def simple_sentence_tokenize(text):
    \"\"\"Tokenisasi kalimat sederhana tanpa NLTK\"\"\"
    # Bersihkan teks terlebih dahulu
    text = text.replace('\\n', ' ').strip()

    # Pisahkan berdasarkan tanda baca umum
    sentences = []
    for part in re.split(r'(?<=[.!?])\\s+', text):
        if part.strip():
            sentences.append(part.strip())

    # Jika tidak ada kalimat yang ditemukan, kembalikan seluruh teks sebagai satu kalimat
    if not sentences:
        return [text]

    return sentences

def tokenize_words(text):
    \"\"\"Tokenisasi kata sederhana tanpa NLTK\"\"\"
    text = text.lower()
    # Bersihkan teks
    text = re.sub(r'[^\\w\\s]', ' ', text)
    # Split kata-kata
    return [word for word in text.split() if word.strip()]
""")

    print("Fungsi pemrosesan teks berhasil disimpan!")
except Exception as e:
    print(f"Error saat menyimpan fungsi pemrosesan teks: {str(e)}")

"""# BAGIAN 9: MODUL RINGKASAN TEKS UNTUK PENGGUNAAN EKSTERNAL"""

print("\n" + "=" * 60)
print("MEMBUAT MODUL RINGKASAN TEKS UNTUK PENGGUNAAN EKSTERNAL")
print("=" * 60)

try:
    print("Membuat modul ringkasan teks...")

    with open("summarizer.py", "w") as f:
        f.write("""
import tensorflow as tf
import numpy as np
import pickle
import re
import os

# Import fungsi pemrosesan teks jika tersedia
try:
    from text_processing import clean_text, simple_sentence_tokenize, tokenize_words
except ImportError:
    # Definisi fungsi inline jika modul tidak tersedia
    def clean_text(text):
        \"\"\"Pembersihan teks yang lebih robust\"\"\"
        if not isinstance(text, str):
            return ""

        # Remove extra whitespaces
        text = re.sub(r'\\s+', ' ', text)

        # Remove special characters but keep punctuation
        text = re.sub(r'[^\\w\\s.,!?;:\\-()]', '', text)

        # Remove multiple punctuation
        text = re.sub(r'[.,!?;:]{2,}', '.', text)

        return text.strip()

    def simple_sentence_tokenize(text):
        \"\"\"Tokenisasi kalimat sederhana tanpa NLTK\"\"\"
        # Bersihkan teks terlebih dahulu
        text = text.replace('\\n', ' ').strip()

        # Pisahkan berdasarkan tanda baca umum
        sentences = []
        for part in re.split(r'(?<=[.!?])\\s+', text):
            if part.strip():
                sentences.append(part.strip())

        # Jika tidak ada kalimat yang ditemukan, kembalikan seluruh teks sebagai satu kalimat
        if not sentences:
            return [text]

        return sentences

    def tokenize_words(text):
        \"\"\"Tokenisasi kata sederhana tanpa NLTK\"\"\"
        text = text.lower()
        # Bersihkan teks
        text = re.sub(r'[^\\w\\s]', ' ', text)
        # Split kata-kata
        return [word for word in text.split() if word.strip()]

class TextSummarizer:
    def __init__(self, model_path=None, input_tokenizer_path=None, output_tokenizer_path=None):
        \"\"\"Inisialisasi text summarizer dengan model dan tokenizer opsional\"\"\"
        self.model = None
        self.input_tokenizer = None
        self.output_tokenizer = None
        self.max_input_len = 200

        # Load model dan tokenizer jika path diberikan
        if model_path and os.path.exists(model_path) and input_tokenizer_path and os.path.exists(input_tokenizer_path):
            self.load(model_path, input_tokenizer_path, output_tokenizer_path)

    def load(self, model_path, input_tokenizer_path, output_tokenizer_path=None):
        \"\"\"Load model dan tokenizer dari file\"\"\"
        try:
            # Load model
            self.model = tf.keras.models.load_model(model_path)
            print(f"Model berhasil dimuat dari {model_path}")

            # Load input tokenizer
            with open(input_tokenizer_path, 'rb') as handle:
                self.input_tokenizer = pickle.load(handle)
            print(f"Input tokenizer berhasil dimuat dari {input_tokenizer_path}")

            # Load output tokenizer jika tersedia
            if output_tokenizer_path and os.path.exists(output_tokenizer_path):
                with open(output_tokenizer_path, 'rb') as handle:
                    self.output_tokenizer = pickle.load(handle)
                print(f"Output tokenizer berhasil dimuat dari {output_tokenizer_path}")

            return True
        except Exception as e:
            print(f"Error saat memuat model dan tokenizer: {e}")
            return False

    def predict_sentence_importance(self, sentences):
        \"\"\"Memprediksi pentingnya kalimat menggunakan model\"\"\"
        if self.model is None or self.input_tokenizer is None:
            raise ValueError("Model atau tokenizer belum dimuat")

        # Tokenize dan pad setiap kalimat
        sequences = []
        for sentence in sentences:
            seq = self.input_tokenizer.texts_to_sequences([sentence])
            if seq[0]:  # Jika tidak kosong
                padded_seq = tf.keras.preprocessing.sequence.pad_sequences(
                    seq, maxlen=self.max_input_len, padding='post'
                )
                sequences.append(padded_seq)
            else:
                # Jika tokenisasi gagal, beri nilai 0
                sequences.append(np.zeros((1, self.max_input_len)))

        # Prediksi skor penting untuk setiap kalimat
        importance_scores = []
        for seq in sequences:
            score = self.model.predict(seq, verbose=0)[0][0]
            importance_scores.append(score)

        return importance_scores

    def summarize(self, text, max_sentences=3):
        \"\"\"Ringkas teks menggunakan model atau pendekatan ekstraktif\"\"\"
        # Preprocessing
        cleaned_text = clean_text(text)
        if not cleaned_text:
            return "Teks tidak valid atau kosong."

        # Tokenisasi kalimat
        try:
            # Coba gunakan NLTK jika tersedia
            import nltk
            from nltk.tokenize import sent_tokenize
            nltk.download('punkt', quiet=True)
            sentences = sent_tokenize(cleaned_text)
        except:
            # Fallback ke tokenisasi sederhana
            sentences = simple_sentence_tokenize(cleaned_text)

        # Jika teks sudah pendek, return as is
        if len(sentences) <= max_sentences:
            return cleaned_text

        # Gunakan model untuk memprediksi kalimat penting jika tersedia
        if self.model is not None and self.input_tokenizer is not None:
            try:
                importance_scores = self.predict_sentence_importance(sentences)

                # Ambil indeks kalimat dengan skor tertinggi
                top_indices = np.argsort(importance_scores)[-max_sentences:]
                top_indices = sorted(top_indices)  # Urutkan berdasarkan posisi asli

                # Ambil kalimat-kalimat penting
                summary_sentences = [sentences[i] for i in top_indices]

                return " ".join(summary_sentences)

            except Exception as e:
                print(f"Error saat prediksi model: {e}")
                # Fallback ke strategi ekstraktif

        # Strategi ekstraktif sederhana (kalimat pertama, tengah, terakhir)
        summary_sentences = [sentences[0]]  # Kalimat pertama selalu penting

        if max_sentences >= 2:
            summary_sentences.append(sentences[-1])  # Kalimat terakhir

        if max_sentences >= 3 and len(sentences) > 2:
            # Tambahkan kalimat tengah
            middle_idx = len(sentences) // 2
            if sentences[middle_idx] not in summary_sentences:
                summary_sentences.insert(1, sentences[middle_idx])

        # Urutkan berdasarkan posisi asli dalam teks
        positions = []
        for sent in summary_sentences:
            for i, s in enumerate(sentences):
                if sent == s:
                    positions.append(i)
                    break

        sorted_pairs = sorted(zip(positions, summary_sentences))
        ordered_summary = [pair[1] for pair in sorted_pairs]

        return " ".join(ordered_summary)

def summarize_text(text, max_sentences=3):
    \"\"\"Fungsi praktis untuk meringkas teks tanpa memerlukan model\"\"\"
    # Preprocessing
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return "Teks tidak valid atau kosong."

    # Tokenisasi kalimat
    sentences = simple_sentence_tokenize(cleaned_text)

    # Jika teks sudah pendek, return as is
    if len(sentences) <= max_sentences:
        return cleaned_text

    # Strategi ekstraktif sederhana (kalimat pertama, tengah, terakhir)
    summary_sentences = [sentences[0]]  # Kalimat pertama selalu penting

    if max_sentences >= 2:
        summary_sentences.append(sentences[-1])  # Kalimat terakhir

    if max_sentences >= 3 and len(sentences) > 2:
        # Tambahkan kalimat tengah
        middle_idx = len(sentences) // 2
        if sentences[middle_idx] not in summary_sentences:
            summary_sentences.insert(1, sentences[middle_idx])

    # Urutkan berdasarkan posisi asli dalam teks
    positions = []
    for sent in summary_sentences:
        for i, s in enumerate(sentences):
            if sent == s:
                positions.append(i)
                break

    sorted_pairs = sorted(zip(positions, summary_sentences))
    ordered_summary = [pair[1] for pair in sorted_pairs]

    return " ".join(ordered_summary)

# Contoh penggunaan
if __name__ == "__main__":
    # Contoh teks
    sample_text = '''
    Pemerintah Indonesia telah mengumumkan rencana pembangunan ibu kota baru di Kalimantan Timur.
    Keputusan ini diambil setelah melalui studi yang panjang terkait berbagai aspek, termasuk
    ketahanan terhadap bencana, ketersediaan lahan, dan potensi ekonomi. Ibu kota baru ini diharapkan
    dapat mengurangi kepadatan di Jakarta dan mendistribusikan pembangunan ekonomi secara lebih merata.
    Proyek ambisius ini membutuhkan investasi besar dan akan dilaksanakan secara bertahap dalam
    jangka waktu beberapa tahun. Para ahli menyatakan bahwa perpindahan ibu kota ini juga akan
    membawa tantangan tersendiri, terutama dalam hal infrastruktur dan adaptasi masyarakat.
    '''

    # Ringkas teks dengan fungsi sederhana
    print("\\nTeks asli:\\n", sample_text)
    print("\\nRingkasan sederhana:\\n", summarize_text(sample_text))

    # Coba load model dan ringkas teks
    try:
        # Cari file model dan tokenizer di direktori saat ini
        files = os.listdir('.')
        model_file = next((f for f in files if f.startswith('text_summarizer_model') and (f.endswith('.keras') or f.endswith('.h5'))), None)
        input_tokenizer_file = 'input_tokenizer.pickle' if 'input_tokenizer.pickle' in files else None

        if model_file and input_tokenizer_file:
            summarizer = TextSummarizer(
                model_path=model_file,
                input_tokenizer_path=input_tokenizer_file
            )

            print("\\nRingkasan dengan model:\\n", summarizer.summarize(sample_text))
        else:
            print("\\nFile model atau tokenizer tidak ditemukan.")
    except Exception as e:
        print(f"\\nTidak dapat menggunakan model: {e}")
""")

    print("Modul ringkasan teks berhasil dibuat!")
except Exception as e:
    print(f"Error saat membuat modul ringkasan teks: {str(e)}")

"""# BAGIAN 10: DEMO PENGGUNAAN MODEL DENGAN ANTARMUKA INTERAKTIF"""

print("\n" + "=" * 60)
print("DEMO PENGGUNAAN MODEL DENGAN ANTARMUKA INTERAKTIF")
print("=" * 60)

from google.colab import output
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output

# Membuat tampilan untuk demo interaktif
def create_interactive_demo():
    # Membuat widgets
    text_area = widgets.Textarea(
        value='',
        placeholder='Masukkan teks di bawah ini',
        description='',
        disabled=False,
        layout=widgets.Layout(width='100%', height='200px')
    )

    max_sentences = widgets.IntSlider(
        value=3,
        min=1,
        max=5,
        step=1,
        description='Jumlah kalimat:',
        disabled=False,
        continuous_update=False,
        orientation='horizontal',
        readout=True,
        readout_format='d'
    )

    summarize_button = widgets.Button(
        description='RINGKAS TEKS',
        button_style='success',
        tooltip='Klik untuk meringkas teks',
        icon='check'
    )

    output_area = widgets.Output(
        layout=widgets.Layout(width='100%', border='1px solid #ddd', padding='10px')
    )

    statistics_output = widgets.Output(
        layout=widgets.Layout(width='100%', border='1px solid #ddd', padding='10px', margin='10px 0')
    )

    # Fungsi callback untuk tombol summarize
    def on_summarize_button_clicked(b):
        with output_area:
            clear_output()
            print("Sedang meringkas teks...")

            input_text = text_area.value
            if not input_text.strip():
                print("Teks kosong! Silakan masukkan teks terlebih dahulu.")
                return

            # Meringkas teks
            summary = summarize_text(input_text, max_sentences.value)

            # Menghitung statistik
            original_words = len(input_text.split())
            summary_words = len(summary.split())
            compression_ratio = (summary_words / original_words * 100) if original_words > 0 else 0

            clear_output()
            print("HASIL RINGKASAN:")
            print("=" * 45)
            print(summary)
            print("=" * 45)

            # Menampilkan statistik
            with statistics_output:
                clear_output()
                print("STATISTIK:")
                print(f"â€¢ Teks asli: {original_words} kata")
                print(f"â€¢ Ringkasan: {summary_words} kata")
                print(f"â€¢ Rasio kompresi: {compression_ratio:.1f}%")

    # Menghubungkan fungsi callback dengan tombol
    summarize_button.on_click(on_summarize_button_clicked)

    # Menampilkan petunjuk
    display(HTML("""
    <div style="background-color: #f8f9fa; padding: 10px; margin-bottom: 10px; border-left: 5px solid #007bff;">
        <h3 style="margin-top: 0;">PERINGKAS TEKS BAHASA INDONESIA</h3>
        <p>=============================================</p>
        <ol>
            <li>Masukkan teks di bawah ini</li>
            <li>Klik tombol "RINGKAS TEKS"</li>
            <li>Hasil akan muncul di bawah tombol</li>
        </ol>
    </div>
    """))

    # Menampilkan contoh teks
    example_button = widgets.Button(
        description='CONTOH TEKS (copy-paste ke text area):',
        button_style='info',
        tooltip='Klik untuk melihat contoh teks',
    )

    example_output = widgets.Output(
        layout=widgets.Layout(width='100%', border='1px solid #ddd', padding='10px', margin='5px 0', max_height='100px', overflow='auto')
    )

    with example_output:
        print("Cirebon, Jawa Barat (ANTARA News) - Sebanyak 30 karyawan mengalami keracunan setelah terjadi kebocoran pipa gas amonia di PT Suri Tani Pemuka, di Kabupaten Cirebon, Jawa Barat, Sabtu. Kebocoran saluran gas amonia yang terjadi di PT Suri Tani Pemuka tersebut sekitar pukul 13.00 WIB menyebabkan 30 pegawai mengalami keracunan. Kabit Humas Polda Jawa Barat, Komisaris Besar Polisi Yusri Yunus, saat dihubungi dari Cirebon, Sabtu, Yunus mengatakan dari keterangan saksi bahwa pada saat dirinya bekerja di pabrik tersebut tiba-tiba mencium bau tidak sedap. Dari bau tersebut, kata Yunus, mengakibatkan para karyawan mengalami sesak napas dan pusing setelah menghirup gas amonia tersebut. Menurut dia, akibat kejadian itu sebanyak 30 orang menjadi korban dan langsung dilarikan ke IGD Rumah Sakit Pelabuhan Cirebon (RSPC) untuk mendapatkan perawatan. \"Perusahaan tersebut juga telah melakukan upaya penutupan pipa dan bertanggung jawab atas biaya pengobatan korban,\" kata Yunus. Pihaknya juga telah melakukan olah TKP terhadap kejadian kebocoran gas di pabrik tersebut. Sementara itu, PT Suri Tani Pemuka, bergerak dalam bidang pakan ikan ini, berada di Jalan Raya Sunan Gunung Jati, Desa Bungko Lor, Kecamatan Kapetakan, Kabupaten Cirebon.")

    def on_example_button_clicked(b):
        example_output.layout.display = 'block' if example_output.layout.display == 'none' else 'none'

    example_button.on_click(on_example_button_clicked)

    # Menyusun semua widgets
    display(example_button, example_output)
    display(text_area)
    display(max_sentences)
    display(summarize_button)
    display(output_area)
    display(statistics_output)

# Menjalankan demo interaktif
create_interactive_demo()

from huggingface_hub import login
login(token="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")

import os
import shutil

# Nama folder tujuan
folder_name = "summarizer_model_fix"

# Buat folder kalau belum ada
if not os.path.exists(folder_name):
    os.makedirs(folder_name)

# Daftar file yang ingin dipindahkan
files_to_move = [
    "best_summarization_model.h5",
    "input_tokenizer.pickle",
    "output_tokenizer.pickle",
    "text_summarizer_model.keras",
    "summarizer.py",
    "text_processing.py"
]

# Pindahkan file ke folder
for file in files_to_move:
    if os.path.exists(file):
        shutil.move(file, os.path.join(folder_name, file))
    else:
        print(f"File {file} tidak ditemukan.")

print("Semua file berhasil dipindahkan ke folder:", folder_name)

from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="/content/summarizer_model_fix",
    repo_id="fransiskaarthaa/summarizereal-JS",
    repo_type="model"
)