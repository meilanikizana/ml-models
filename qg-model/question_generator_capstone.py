# -*- coding: utf-8 -*-
"""Final Question Generator - Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E7f3XrkS9XzmPJNxsI9zLflSH2FrJynC

# Question Generator (QG) Model Using HuggingFace T5

## Install and Import Needed Library
"""

!pip install transformers datasets sentencepiece

!pip install transformers seqeval

!pip install rouge-score

from google.colab import files
import pandas as pd
import random
from datasets import Dataset
import torch
import re
from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from rouge_score import rouge_scorer

"""## Data Loading"""

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mintupsidup/squad-variated-indo
!unzip squad-variated-indo.zip

train = pd.read_json('train-v2.0.json', orient='column')
dev = pd.read_json('dev-v2.0.json', orient='column')

init_df = pd.concat([train, dev], ignore_index=True)
init_df

init_df.info()

"""## Data Preparation

### Extract Features
"""

contexts, questions, answers = [], [], []

for data in init_df['data']:
    for para in data['paragraphs']:
        context = para['context']
        for all_questions in para['qas']:
            if not all_questions.get("is_impossible", False):
                question = all_questions['question']
                answer_list = all_questions['answers']
                if len(answer_list) > 0:
                    answer = answer_list[0]['text']
                    questions.append(question)
                    contexts.append(context)
                    answers.append(answer)

print(f"Total data pertanyaan: {len(questions)}")

df = pd.DataFrame({
    "context": contexts,
    "question": questions,
    "answer": answers
})

df

"""### Handling Missing Values and Duplicated Data"""

df.info()

print("Missing values:\n", df.isnull().sum())
print("\nDuplikasi baris:", df.duplicated().sum())

df.drop_duplicates(inplace=True)
df.reset_index(drop=True, inplace=True)
df

print("Duplikasi baris:", df.duplicated().sum())

"""### Data Sampling, Convert to HuggingFace Dataset & Split to Train and Test"""

sample_size = 1000
df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)

contexts_list = df_sample['context'].tolist()
questions_list = df_sample['question'].tolist()
answers_list = df_sample['answer'].tolist()

raw_dataset = Dataset.from_dict({
    "context": contexts_list,
    "question": questions_list,
    "answer": answers_list
})

raw_dataset = raw_dataset.train_test_split(test_size=0.2)
raw_dataset

"""### Preprocessing and Tokenization"""

tokenizer = T5Tokenizer.from_pretrained("cahya/t5-base-indonesian-summarization-cased")

def preprocess(text):
    input_text = f"generate question: context: {text['context']} answer: {text['answer']}"
    target_text = text["question"]

    model_inputs = tokenizer(
        input_text, max_length=512, padding="max_length", truncation=True
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            target_text, max_length=64, padding="max_length", truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


tokenized_dataset = raw_dataset.map(preprocess, batched=False)

"""## Load and Configure T5 Model"""

t5_model = T5ForConditionalGeneration.from_pretrained("cahya/t5-base-indonesian-summarization-cased")

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=1,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    report_to="none",
    fp16=True
)

trainer = Trainer(
    model=t5_model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

trainer.train()

"""## Load Indonesian Named Entity Recognition (NER) Model"""

ner_tokenizer = AutoTokenizer.from_pretrained("cahya/bert-base-indonesian-NER")
ner_model = AutoModelForTokenClassification.from_pretrained("cahya/bert-base-indonesian-NER")

ner_pipe = pipeline("ner", model=ner_model, tokenizer=ner_tokenizer, grouped_entities=True)

"""## Functions"""

def extract_entities(text, max_entities=5):
    entities = ner_pipe(text)
    answers = []
    for entity in entities:
        if entity['entity_group'] in ['PER', 'LOC', 'DAT']:
            word = entity['word'].replace('##', '').strip()
            if word not in answers:
                answers.append(word)
        if len(answers) >= max_entities:
            break
    return answers

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
t5_model.to(device)

def get_sentence_with_answer(text, answer):
    sentences = re.split(r'(?<=[.!?]) +', text)
    for sentence in sentences:
        if answer in sentence:
            return sentence
    return text

def generate_question_from_input(input_text):
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    ).to(device)

    outputs = t5_model.generate(
        inputs["input_ids"],
        max_length=64,
        num_return_sequences=1,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.8
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def generate_questions(text, num_questions=5):
    answers = extract_entities(text, max_entities=num_questions)
    questions = []

    if answers:
        for ans in answers:
            context = get_sentence_with_answer(text, ans)
            input_text = f"generate question: context: {context} answer: {ans}"
            question = generate_question_from_input(input_text)
            questions.append(question)
            remaining_questions = num_questions - len(questions)
        if remaining_questions > 0:
            for i in range(remaining_questions):
                input_text = f"generate question: context: {text}"
                question = generate_question_from_input(input_text)
                questions.append(question)
    else:
        for _ in range(num_questions):
            input_text = f"generate question: context: {text}"
            question = generate_question_from_input(input_text)
            questions.append(question)

    return questions

"""## Inference Process"""

text = "Hari lahir Pancasila jatuh pada tanggal 1 Juni yang ditandai oleh pidato yang dilakukan oleh Presiden pertama Indonesia Ir Soekarno pada tanggal 1 Juni 1945 dalam sidang Dokuritsu Junbi Cosakai (Badan Penyelidik Usaha Persiapan Kemerdekaan). Dalam pidatonya pertama kali mengemukakan konsep awal Pancasila yang menjadi dasar negara Indonesia. Adapun sejarahnya berawal dari kekalahan Jepang pada perang pasifik, mereka kemudian berusaha mendapatkan hati masyarakat dengan menjanjikan kemerdekaan kepada Indonesia dan membentuk sebuah Lembaga yang tugasnya untuk mempersiapkan hal tersebut. Lembaga ini dinamai Dokuritsu Junbi Cosakai. Pada sidang pertamanya di tanggal 29 Mei 1945 yang diadakan di Gedung Chuo Sangi In (sekarang Gedung Pancasila), para anggota membahas mengenai tema dasar negara. Sidang berjalan sekitar hampir 5 hari, kemudian pada tanggal 1 Juni 1945, Soekarno menyampaikan ide serta gagasannya terkait dasar negara Indonesia, yang dinamai “Pancasila”. Panca artinya lima, sedangkan sila artinya prinsip atau asas."
questions = generate_questions(text, num_questions=5)

for i, q in enumerate(questions, 1):
    print(f"{i}. {q}")

"""## Evaluation"""

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

"""### Data Train 500"""

sample_data = raw_dataset['train'][500]
context_500 = sample_data['context']
reference_question_500 = sample_data['question']

print("Konteks:\n", context_500)
print("\nPertanyaan:\n", reference_question_500)

questions_500 = generate_questions(context_500, num_questions=5)
generated_question_500 = []

for i, q in enumerate(questions_500, 1):
    generated_question_500.append(q)
    print(f"{i}. {q}")

for i, q in enumerate(generated_question_500, 1):
    scores = scorer.score(reference_question_500, q)
    print(f"{i}. {scores}")

"""### Data Train 250"""

sample_data = raw_dataset['train'][250]
context_250 = sample_data['context']
reference_question_250 = sample_data['question']

print("Konteks:\n", context_250)
print("\nPertanyaan:\n", reference_question_250)

questions_250 = generate_questions(context_250, num_questions=5)
generated_question_250 = []

for i, q in enumerate(questions_250, 1):
    generated_question_250.append(q)
    print(f"{i}. {q}")

for i, q in enumerate(generated_question_250, 1):
    scores = scorer.score(reference_question_250, q)
    print(f"{i}. {scores}")

"""### Data Train 150"""

sample_data = raw_dataset['train'][150]
context_150 = sample_data['context']
reference_question_150 = sample_data['question']

print("Konteks:\n", context_150)
print("\nPertanyaan:\n", reference_question_150)

questions_150 = generate_questions(context_150, num_questions=5)
generated_question_150 = []

for i, q in enumerate(questions_150, 1):
    generated_question_150.append(q)
    print(f"{i}. {q}")

for i, q in enumerate(generated_question_150, 1):
    scores = scorer.score(reference_question_150, q)
    print(f"{i}. {scores}")

"""### Data Train 50"""

sample_data = raw_dataset['train'][50]
context_50 = sample_data['context']
reference_question_50 = sample_data['question']

print("Konteks:\n", context_50)
print("\nPertanyaan:\n", reference_question_50)

questions_50 = generate_questions(context_50, num_questions=5)
generated_question_50 = []

for i, q in enumerate(questions_50, 1):
    generated_question_50.append(q)
    print(f"{i}. {q}")

for i, q in enumerate(generated_question_50, 1):
    scores = scorer.score(reference_question_50, q)
    print(f"{i}. {scores}")

"""### Data Train 5"""

sample_data = raw_dataset['train'][5]
context_5 = sample_data['context']
reference_question_5 = sample_data['question']

print("Konteks:\n", context_5)
print("\nPertanyaan:\n", reference_question_5)

questions_5 = generate_questions(context_5, num_questions=5)
generated_question_5 = []

for i, q in enumerate(questions_5, 1):
    generated_question_5.append(q)
    print(f"{i}. {q}")

for i, q in enumerate(generated_question_5, 1):
    scores = scorer.score(reference_question_5, q)
    print(f"{i}. {scores}")

"""## Save Pretrained Model and Tokenizer"""

t5_model.save_pretrained("qg_model")
tokenizer.save_pretrained("qg_model")