# -*- coding: utf-8 -*-
"""Copy of Question Generator - Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E7f3XrkS9XzmPJNxsI9zLflSH2FrJynC
"""

!pip install transformers datasets sentencepiece

!pip install transformers seqeval

from google.colab import files
import pandas as pd
import random
from datasets import Dataset
import torch
import re
from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from transformers import MarianMTModel, MarianTokenizer

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mintupsidup/squad-variated-indo
!unzip squad-variated-indo.zip

train = pd.read_json('train-v2.0.json', orient='column')
dev = pd.read_json('dev-v2.0.json', orient='column')
init_df = pd.concat([train, dev], ignore_index=True)
init_df

init_df.info()

contexts, questions, answers = [], [], []

for data in init_df['data']:
    for para in data['paragraphs']:
        context = para['context']
        for all_questions in para['qas']:
            if not all_questions.get("is_impossible", False):
                question = all_questions['question']
                answer_list = all_questions['answers']
                if len(answer_list) > 0:
                    answer = answer_list[0]['text']
                    questions.append(question)
                    contexts.append(context)
                    answers.append(answer)

print(f"Total data pertanyaan: {len(questions)}")

df = pd.DataFrame({
    "context": contexts,
    "question": questions,
    "answer": answers
})
df

df.info()

print("Missing values:\n", df.isnull().sum())
print("\nDuplikasi baris:", df.duplicated().sum())

df.drop_duplicates(inplace=True)
df.reset_index(drop=True, inplace=True)
df

print("Duplikasi baris:", df.duplicated().sum())

copy_df = df.copy()
copy_df['context_len'] = copy_df['context'].apply(lambda x: len(x.split()))
copy_df['question_len'] = copy_df['question'].apply(lambda x: len(x.split()))
copy_df['answer_len'] = copy_df['answer'].apply(lambda x: len(x.split()))

print(copy_df[['context_len', 'question_len', 'answer_len']].describe())

sample_size = 1000
df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)

contexts_clean = df_sample['context'].tolist()
questions_clean = df_sample['question'].tolist()
answers_clean = df_sample['answer'].tolist()

raw_dataset = Dataset.from_dict({
    "context": contexts_clean,
    "question": questions_clean,
    "answer": answers_clean
})

raw_dataset = raw_dataset.train_test_split(test_size=0.2)
raw_dataset

tokenizer = T5Tokenizer.from_pretrained("t5-small")

def preprocess(text):
    input_text = f"generate question: context: {text['context']} answer: {text['answer']}"
    target_text = text["question"]

    model_inputs = tokenizer(
        input_text, max_length=512, padding="max_length", truncation=True
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            target_text, max_length=64, padding="max_length", truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


tokenized_dataset = raw_dataset.map(preprocess, batched=False)

t5_model = T5ForConditionalGeneration.from_pretrained("t5-small")

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    report_to="none",
    fp16=True
)

trainer = Trainer(
    model=t5_model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

trainer.train()

ner_tokenizer = AutoTokenizer.from_pretrained("cahya/bert-base-indonesian-NER")
ner_model = AutoModelForTokenClassification.from_pretrained("cahya/bert-base-indonesian-NER")

ner_pipe = pipeline("ner", model=ner_model, tokenizer=ner_tokenizer, grouped_entities=True)

def extract_entities(text, max_entities=5):
    entities = ner_pipe(text)
    answers = []
    for entity in entities:
        if entity['entity_group'] in ['PER', 'LOC', 'DAT']:
            word = entity['word'].replace('##', '').strip()
            if word not in answers:
                answers.append(word)
        if len(answers) >= max_entities:
            break
    return answers

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
t5_model.to(device)

def get_sentence_with_answer(text, answer):
    sentences = re.split(r'(?<=[.!?]) +', text)
    for sentence in sentences:
        if answer in sentence:
            return sentence
    return text

def generate_questions(text, num_questions=5):
    answers = extract_entities(text, max_entities=num_questions)
    questions = []

    for ans in answers:
        context = get_sentence_with_answer(text, ans)

        input_text = f"generate question: {context} answer: {ans}"

        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(device)

        outputs = t5_model.generate(
            inputs["input_ids"],
            max_length=64,
            num_return_sequences=1,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.8
        )

        question = tokenizer.decode(outputs[0], skip_special_tokens=True)
        questions.append((question))

    return questions

text = "Hari lahir Pancasila jatuh pada tanggal 1 Juni yang ditandai oleh pidato yang dilakukan oleh Presiden pertama Indonesia Ir Soekarno pada tanggal 1 Juni 1945 dalam sidang Dokuritsu Junbi Cosakai (Badan Penyelidik Usaha Persiapan Kemerdekaan). Dalam pidatonya pertama kali mengemukakan konsep awal Pancasila yang menjadi dasar negara Indonesia. Adapun sejarahnya berawal dari kekalahan Jepang pada perang pasifik, mereka kemudian berusaha mendapatkan hati masyarakat dengan menjanjikan kemerdekaan kepada Indonesia dan membentuk sebuah Lembaga yang tugasnya untuk mempersiapkan hal tersebut. Lembaga ini dinamai Dokuritsu Junbi Cosakai. Pada sidang pertamanya di tanggal 29 Mei 1945 yang diadakan di Gedung Chuo Sangi In (sekarang Gedung Pancasila), para anggota membahas mengenai tema dasar negara. Sidang berjalan sekitar hampir 5 hari, kemudian pada tanggal 1 Juni 1945, Soekarno menyampaikan ide serta gagasannya terkait dasar negara Indonesia, yang dinamai “Pancasila”. Panca artinya lima, sedangkan sila artinya prinsip atau asas."
questions = generate_questions(text, num_questions=5)
for i, q in enumerate(questions, 1):
    print(f"{i}. {q}")

t5_model.save_pretrained("qg_model")
tokenizer.save_pretrained("qg_model")